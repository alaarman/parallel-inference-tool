%!TEX root = main.tex

\section{Probabilistic Inference using Decision Diagrams}\label{sec:parallel}

Bayesian networks represent concise factorizations of probability distributions by using conditional independence assumptions. The size of the factorization has direct implications toward the cost of reasoning, i.e., probabilistic inference. A more expressive model must be used to further improve a BN's factorization in order to exploit additional independences~\cite{boutilier1996context,friedman1998learning,zhang1996exploiting}. A prominent way of achieving this is by using Boolean algebra to find a more concise and canonical representation, such as a Binary Decision Diagram (BDD)~\cite{bryant1986graph}. Compiling a BN into a decision diagram (DD) representations is commonly referred to as \emph{knowledge compilation}~\cite{darwiche2002knowledge}, or simply compilation.


\begin{figure}[!t]
    \centering

    \scalebox{0.85}{
    \input{figures/completeoverview.tex}
    }
    \caption{The compositional framework.}
    \label{fig:frameworkoverview}
\end{figure}

Figure~\ref{fig:frameworkoverview} shows an overview of the underlying principles behind probabilistic inference through knowledge compilation, divided into two steps: \emph{compilation} and \emph{inference}. We will dive into these steps in the following sections without going into too much detail. In-depth descriptions can be found in~\cite{dal2021compositional}. The concrete implementation of \toolname is described in Section~\ref{sec:tool}.

\subsection{Compilation}

We first describe compilation as all steps required to obtain a DD. In addition to traditional compilation, we introduce partitioning to further improve overall performance~\cite{dal2017reducing}.
A partitioning is found that decomposes the BN into independent components. With the partitioning in hand, the following steps can be performed independently, per partition. Each partition is considered an independent BN from this point on.

BNs are defined over multi-valued domains. Prior to compiling it to a DD, we require an encoding to transition from the multi-valued domain to the Boolean domain. There are multiple ways to do this. We choose to first translate a BN into a Boolean formula with dedicated variables to represent probabilities~\cite{chavira2008probabilistic,dal2017wpbdd}.
% (in this step we do not need to introduce extra variables as e.g. a Tseitin transformation would).

The obtained formula serves as an entry point for the language compiler~\cite{dudek2020addmc}. These compilers target different variations of DDs. The process of compiling into a respective DD is by far the most expensive operation of WMC as a whole. Performance is primarily determined by this step, while inference is much cheaper as desired.

With monolithic compilation, we would only be able to amortize the cost of compilation by performing many inference queries. With partitioned compilation, we shift some of this cost over to the inference side, yielding overall performance improvements in cases where we would traditionally not be able to achieve sufficient amortization~\cite{dal2017reducing}. We have now reached the end of the compilation part as indicated by Figure~\ref{fig:frameworkoverview}.

\subsection{Inference}\label{subsec:inference}

After compilation, we arrive at the inference step. The computational complexity of inference is linear in the size of the target representation~\cite{darwiche2002knowledge}. Inference is performed through \emph{Weighted Model Counting} (WMC)~\cite{chavira2008probabilistic,darwiche2021quantifying} on the DD.

In case we chose to partition the BN during the compilation step, we have to adapt the WMC step to recombine the compiled DDs. A partition's representation should be connected to another when they share a common variable. This implies that we need to traverse partitions according to some partial order. It can be represented by a tree, which we suitably refer to as a composition-tree~\cite{dal2021compositional}. The order in which we choose to traverse partitions  (combined with common variable relations) determines how they are connected. As we traverse one partition, its sink is connected to the next partition's root. Now that all partitions form one connected component, we can proceed as previously described with a traversal we are already familiar with from WMC. We discuss how partitioning influence inference complexity in Section~\ref{sec:conclusion}.

\section{\toolname}
\label{sec:tool}\label{sec:implementation}

We present \toolname in Figure~\ref{fig:implementation}. It implements the principles outlined in Figure~\ref{fig:frameworkoverview} and Section~\ref{sec:parallel}. \toolname is a collection of two applications written in C++. The compilation step is implemented by the \verb+COMPILER+, whereas the inference step is implemented by the \engine. We provide a high-level view of the implementations in the following sections.
In-depth descriptions on the used partitioning technique are provided in~\cite{dal2017reducing,dal2021compositional}, and how they created independencies are exploited through parallelism in~\cite{dal2018parallel}.

\begin{figure}[!t]
    \centering
    \scalebox{0.9}{
    \input{figures/implementation.tex}
    }
    \caption{The implementation.}
    \label{fig:implementation}
\end{figure}

\subsection{The \texttt{COMPILER}}

As shown in Figure~\ref{fig:implementation}, the \compiler takes as input a BN encoded in the HUGIN format~\cite{madsen2003hugin}. The compiler is responsible for creating a decision diagram from the provided BN and writes these to output files. The principles outlined in Section~\ref{sec:parallel} provide an orthogonal framework with regard to the target representation. However, \toolname chooses to target \emph{Weighted Positive Binary Decision Diagrams} (WPBDD), because it is a dedicated representation for probabilistic inference. (We discuss differences with other representations in Section~\ref{sec:conclusion}.)

The \compiler can introduce a partitioning to further improve overall performance~\cite{dal2017reducing}. With a user-provided number of partitions, a partitioning is found for the BN. Using simulated annealing, this partitioning is optimized by minimizing the sum of the  tree-width of all partitions. Tree-width is a metric commonly used to indicate the complexity of BNs~\cite{chen2022definition}. Optionally, a partitioning can also be provided by the user. With the partitioning in hand, the following steps can be performed in parallel, per partition. Theoretically, compilation is as fast as the slowest compiling partition~\cite{dal2018parallel}.

Compilation is critically dependent upon a good variable ordering. The size of the resulting WPBDD (or any other ordered DD) is determined by it. The \compiler can optionally be provided with a variable ordering as input. When not provided by one, an ordering is created automatically using the min-fill heuristic~\cite{chavira2008probabilistic}. Further refinement can be attained when the user requires it. It is achieved by minimizing the tree-width of candidate orderings, using simulated annealing.

\subsection{The \texttt{INFERENCE ENGINE}}

The \engine takes as input a BN, probabilistic queries, and the \compiler's output (Figure~\ref{fig:implementation}). At its core, the \engine is able to perform probabilistic inference through marginalization. It does so using WMC (Section~\ref{subsec:inference}. Given a user-provided set of probabilistic queries, the corresponding marginalizations are performed. Prior to every WMC run, we need to instantiate the variables in the WPBDD, reflecting the marginalization we want to achieve. Proper variable instantiations in the WPBDD are attained by using the \compiler's variable mapping to map BN variables to WPBDD variables.

In extension, we are able to compute conditional probabilities through Bayes' theorem (Section~\ref{sec:background}). Let's say we are interested in probability $P(X = x | E = e) = \frac{P(X = x, E = e)}{P(E = e)}$.
We compute this using two marginalizations, i.e., we run WMC two times. Once for $P(X = x, E = e)$ and once for $P(E = e)$. Now let's say that we want to know the posterior probabilities for every of $X$'s $n$ values. Implementing this na\"ively would result in two marginalizations for each value, yielding a total of $2n$ marginalizations. We reduce this number by reusing the divisor for every computation, as it is unchanging. The divisor $P(E = e)$ is computed once, whereas the denominator $P(X = x, E = e)$ is adjusted for every value. This results in $n + 1$ marginalizations. To further reduce this further, we also use the fact that marginalizing over $X$ sums to $1$. In other words, we only need to compute the posteriors for $n-1$ values. The posterior probability of the last value can be obtained by subtracting the sum of the previous marginalizations from $1$. We therefore only need $n$ marginalizations to compute every posterior for variable $X$.

Notice that each marginalization is an independent run of WMC, just with different variable instantiations. A trivial optimization here is that we can run every call to WMC in \emph{parallel}. Theoretically reducing the cost of computing the posterior probabilities for all instantiations of unobserved variables combined to the single WMC call that requires the most resources.

Parallelism is notoriously difficult to exploit in sparse graphs such as BNs~\cite{dal2014fast,ceylan2021open}. Introducing a partitioning during the compilation step can achieve the required independence such that parallelism can be exploited at the level of individual WMC runs~\cite{dal2017reducing}. Compiled partitions are composed using a composition-tree. This tree is the structure by which we traverse each partition's corresponding WPBDD~\cite{dal2021compositional}. The independence among a parent's children in the composition-tree can be used to run WMC in those parts in parallel, i.e., independent sub-trees can run in parallel~\cite{dal2018parallel}.


