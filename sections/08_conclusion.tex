%!TEX root = main.tex

\section{Discussion}
\label{sec:conclusion}

\toolname shows that parallelism can benefit knowledge compilation and inference for Bayesian networks. Our chosen parallelization approach is based on partitioning. An approach that has previously been exploited to speed up symbolic model checking~\cite{sylvan-journal,narayan1996partitioned,sahoo2004partitioning}.  We additionally find that the partitioning introduces a tradeoff between compilation times and inference times, sacrificing some performance in inference to gain parallel scalability.

The computational complexity of inference is linear in the size of the target representation~\cite{darwiche2002knowledge}. However, it increases when partitioning is employed. A compiled partition in a composition-tree must be exponentially traversed in the size of the cutset that separates it from its parent. Once for every instantiation of the cutset variables. However, this is compensated by a number of principles. $(1)$ the combined size of all partition BDDs is significantly reduced compared to the monolithically compiled BDD~\cite{dal2017reducing}; $(2)$ partition BDDs only represent a portion of its total, reducing traversal resources; $(3)$ each child instantiation can be traversed in parallel, potentially reducing traversal cost of exponential traversals to the cost of 1~\cite{dal2021compositional}; $(4)$ as partition BDDs are small, cache locality start to play an important role, giving an advantage over monolithic BDDs~\cite{dal2018parallel}. Beyond the complexity introduced by cutsets, inference remains linear. Small cutsets can therefore play a crucial role in performance. Separate empirical investigations on partitioning and parallelism show their respective contribution to \toolname's performance~\cite{dal2018parallel,dal2021compositional}.

The parallelization approach of \toolname is orthogonal to the chosen target representation of the knowledge base, which we demonstrated by using it for four different target representations~\cite{dal2018parallel}. As a consequence, the approach is to a certain extent orthogonal to the exploitation of local structure~\cite{shih2019compiling} by those representations, as local structure can still be exploited within the partitioned subproblems. For instance, we showed that causal dependence is fully exploited when using decision diagram representations in the partitions.

For target representations, many choices exist~\cite{chavira2008probabilistic,darwiche2011sdd,choi2022fpga}. Since our partitioning technique exploits the treewidth~\cite{chen2022definition} of the representation~\cite[\S 5]{dal2021compositional}, and the algorithms are based on message passing~\cite[\S 4]{dal2021compositional}, other representations like ADD and d-DNNF can be parallelized alike. While our earlier work~\cite{dal2018parallel} compared the performance of \toolname against various of these other representations, showing competitive performance, here we will point out some differences between the other representations and suggest future work.


Like AADD~\cite{sanner2005affine}, and its similar cousins SLDD~\cite{wilson2005decision}, QMDD~\cite{miller2006qmdd} and FEVBDD~\cite{tafertshofer1997factored}, our target representation WPBDD factors out probabilities on the edges of the diagram, resulting in more succinctness than for instance achieved with ADD~\cite{bahar}, QuiDD~\cite{viamontes2003improving} and MTDD~\cite{Clarke2001}. However, unlike AADD, it only factors according to the structure of the Bayesian network, which sacrifices succinctness but ensures exact representation of the floating point numbers. The latter can be quite important in practice, as rounding errors from manipulation operations can quickly propagate in the discrete data structure, resulting in numerical instability~\cite{zulehner2019efficiently,peham2022equivalence,hillmich2022reordering}.


The effects of different variable orders are known to be crucial in many representation languages. Representations like d-DNNF~\cite{chavira2008probabilistic} and SDD~\cite{darwiche2011sdd}, allow more freedom in the order and could potentially improve the results of \toolname.

Early versions of \toolname also tried different parallelization approaches, like the fine-grained task-based scheduling of Sylvan~\cite{van2013multi}, which has shown that good parallel scalability is possible for model checking problems in BDDs, ADDs (also called Multi-Terminal DDs), and MDDs (Sylvan uses a version called List DD~\cite{sylvan-journal}). In future work, we hope to establish why this approach did not yield good performance for knowledge compilation as well.




